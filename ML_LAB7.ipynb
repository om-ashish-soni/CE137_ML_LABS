{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1H5kmhTx_x8ie5jVVrdod0vZSPUDiHewI","authorship_tag":"ABX9TyNWOkchtTeKvlReuLSmYK07"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Perform logistic regression , create hypothesis and loss function "],"metadata":{"id":"-LxuvWoH0OcJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGoiapoK0FPo","executionInfo":{"status":"ok","timestamp":1674552427162,"user_tz":-330,"elapsed":2,"user":{"displayName":"CE137_Om_Soni","userId":"00290265016344326683"}},"outputId":"b69a1122-983d-48eb-8c7d-a6eea2d2c781"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [-inf],\n","        [0.],\n","        [-inf],\n","        [-inf],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [-inf],\n","        [-inf],\n","        [-inf],\n","        [0.]], dtype=torch.float64, grad_fn=<LogBackward0>)\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [nan, nan, nan,  ..., nan, nan, nan],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64,\n","       grad_fn=<AddBackward0>)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import io\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from copy import deepcopy\n","import tensorflow as tf\n","\n","from math import exp\n","location=\"/content/drive/MyDrive/Sem6/ML/LAB7/BuyComputer.csv\"\n","data = pd.read_csv(location, encoding=\"latin\")\n","data.drop(columns=['User ID',],axis=1,inplace=True)\n","# print(data)\n","\n","x=data[data.columns[:-1]]\n","y=data[data.columns[-1]]\n","# print(x)\n","# print(y)\n","\n","sc=StandardScaler()\n","x=sc.fit_transform(x)\n","\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.90, random_state =137)\n","\n","total_features=len(x_train[0])\n","\n","entries=x_train[:,0]\n","\n","# print(len(x_train),len(entries))\n","\n","\n","x_train=np.array(x_train)\n","y_train=np.array(y_train)\n","# print(input)\n","weights=np.array([[2.] for _ in range(total_features)])\n","bias=np.array([[0.2] for _ in range(len(x_train))])\n","# print(weights)\n","# print(bias)\n","# mat=input.dot(weights)\n","\n","\n","x_train=torch.tensor(x_train,requires_grad=False)\n","y_train=torch.tensor(y_train,requires_grad=False)\n","\n","iterations=1\n","for _ in range(iterations):\n","  weights=torch.tensor(weights,requires_grad=True)\n","  bias=torch.tensor(bias,requires_grad=True)\n","\n","  z=torch.matmul(x_train,weights)+bias\n","  z1=torch.exp(-z)\n","  denom=torch.add(z1,1)\n","  y_pred=denom.clone()\n","  y_pred.pow_(-1)\n","  y_pred=y_pred.round()\n","  # print(y_pred)\n","  print(torch.log(y_pred))\n","  # loss=(y_train*torch.log(y_pred))+(torch.add(1.,-y_train)*torch.log(1-y_pred))\n","  first=torch.mul(y_train,torch.log(y_pred))\n","  second=torch.mul(torch.sub(1.,y_train),torch.log(torch.sub(1.,torch.log(y_pred))))\n","  loss=torch.add(first,second)\n","  print(loss)\n","\n","  # print(y_train[0],torch.log(y_pred[0]))\n","  # print(y_train*torch.log(y_pred))"]},{"cell_type":"code","source":["from tensorflow.python.ops.array_ops import zeros_like_v2\n","import numpy as np\n","import pandas as pd\n","import io\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix\n","import tensorflow as tf\n","\n","from math import exp\n","\n","learning_rate=0.001\n","\n","def convert(arr):\n","  return [[elem] for elem in arr]\n","\n","def deconvert(arr):\n","  return [elem[0] for elem in arr]\n","\n","def sigmoid(z):\n","  denom=1+np.exp(-z)\n","  value=1/denom\n","  return value\n","  pass\n","\n","def predict(input,weights,bias):\n","  z=np.dot(input,weights)\n","  z+=bias\n","  pred=sigmoid(z)\n","  return pred\n","\n","def predict_bulk(input,weights,bias):\n","  pred=[]\n","  for sample in input:\n","    pred_i=predict(sample,weights,bias)\n","    pred.append(pred_i)\n","  pred=np.array(pred)\n","  pred=np.round(pred)\n","\n","  return pred\n","  pass\n","\n","def partial_derivate(pred,output,x):\n","  first=(pred-output)\n","  x=convert(x)\n","  der_all=first*x\n","  \n","  der_all=deconvert(der_all)\n","  der_value=sum(der_all)/len(der_all)\n","  return der_value\n","  pass\n","def param_update(features,pred,output,params):\n","  updated_params=[]\n","  for i in range(len(params)):\n","    x_i=np.array(features[:,i])\n","    d_i=partial_derivate(pred,output,x_i)\n","    deduct=learning_rate*d_i\n","    curr_param=params[i]\n","    curr_param=curr_param-deduct\n","    updated_params.append(curr_param)\n","  return updated_params\n","  pass\n","\n","\n","def calculate_loss(pred,output):\n","  # remaining to calculate\n","  pass\n","\n","def calculate_accuracy(y_pred,y_act):\n","  cmat=confusion_matrix(y_pred,y_act)\n","  truth=sum(cmat[i][i] for i in range(len(cmat)))\n","  total=0\n","  for row in cmat:\n","    for elem in row:\n","      total+=elem\n","  return truth/total\n","\n","location=\"/content/drive/MyDrive/Sem6/ML/LAB7/BuyComputer.csv\"\n","data = pd.read_csv(location, encoding=\"latin\")\n","data.drop(columns=['User ID',],axis=1,inplace=True)\n","\n","x=data[data.columns[:-1]]\n","y=data[data.columns[-1]]\n","\n","sc=StandardScaler()\n","x=sc.fit_transform(x)\n","\n","x_train,x_test,y_train,y_test=train_test_split(x,y,test_size = 0.90, random_state =137)\n","\n","total_features=len(x_train[0])\n","\n","entries=x_train[:,0]\n","\n","x_train=np.array(x_train)\n","\n","\n","y_train=convert(y_train)\n","y_train=np.array(y_train)\n","\n","y_test=np.array(convert(y_test))\n","\n","\n","weights=np.array([[2.] for _ in range(total_features)])\n","bias=np.array([0.10])\n","\n","iterations=10000\n","accuracy=0\n","for _ in range(iterations):\n","  pred=predict_bulk(x_train,weights,bias)\n","  weights,bias=param_update(x_train,pred,y_train,[weights,bias])\n","  c_accuracy=calculate_accuracy(pred,y_train)\n","  accuracy=max(accuracy,c_accuracy)\n","\n","print(accuracy)\n","\n","\n","# testing model\n","y_pred=predict_bulk(x_test,weights,bias)\n","test_accuracy=calculate_accuracy(y_pred,y_test)\n","print(test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxePSbaTHSKZ","executionInfo":{"status":"ok","timestamp":1674560768869,"user_tz":-330,"elapsed":10559,"user":{"displayName":"CE137_Om_Soni","userId":"00290265016344326683"}},"outputId":"035a07d2-48a6-4e9a-b17a-6db772879651"},"execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9\n","0.8277777777777777\n"]}]}]}